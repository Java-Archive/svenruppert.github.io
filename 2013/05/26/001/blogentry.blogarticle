<p>Ich habe nun ein paar Tests mir einem realen Datenbestand gemacht. Es handelt sich hierbei um sage und schreibe 2 TB unsortierter privater Photos.</p> <p>Um in diesem Datenbestand noch Ordnung hineinzubekommen bedarf es normalerweise wochenlanger Arbeit. Ich zumindest habe diese Zeit nicht ;-)</p> <p>Es ergeben sich verschiedene Grundgedanken: </p> <p>- Wie werden Duplikate gefunden und wie geht man damit automatisch um?</p> <p>- Ab wann werden manuelle Eingriffe notwendig?</p> <p>- Besonderheit 3D: Es werden teilweise 3D Aufnahmen gemacht die aus zwei einzelnen Bildern bestehen. Diese werden mit zwei verschiedenen Geräten aufgenommen. Das Problem hier ist, dass weder die Bildnummer noch die Zeit der beiden Geräte synchronisiert gewesen sind. Andere Bilder sind schon im mpo – Format abgelegt. Hier sind beide Bilder in einer Datei zusammen gespeichert.</p> <p>Annahme: Die Exif-Informationen in den Bildern sind meistens verlässlich. Im Zweifellsfall wird sich immer für die Orginaldatei entschieden, nachbearbeitete Bilder dürfen verloren gehen. (Dieses beruht auf der Tatsache, das der Photograf mir versicherte, dass die wenigsten Bilder nachbearbeitet worden sind.)</p> <p>&nbsp;</p> <p>Hier nun zum Vorgehen bei den ersten Tests:</p> <p>Es wurden die Dateien aus allen Ordnern in eine Zielstruktur verschoben. Level I Jahresordner, Level II Monatsordner, Level III Tagesordner.</p> <p>Wenn eine Datei mit dem selben Namen schon vorhanden war, wurde geprüft ob der MD5-Wert gleich ist. Wenn ja, wurde die Datei in eine gleich aufgebaute Ordnerstruktur verschoben die im Hauptverzeichnis duplicate aufgebaut worden ist. War der MD5 – Wert nicht gleich, wird die Datei um eine vortlaufende Nummer im Namen erweitert und im selben Verzeichnis gespeichert. Alleine mit diesem Vorgehen wurden schon sehr viele Duplikate entfernt. Als Referenz diehnt das Aufnahmedatum in den Exif-Informationen. </p>  